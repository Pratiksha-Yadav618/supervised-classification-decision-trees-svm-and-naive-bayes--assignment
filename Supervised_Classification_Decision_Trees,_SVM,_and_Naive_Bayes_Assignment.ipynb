{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Classification: Decision Trees, SVM, and Naive Bayes Assignment"
      ],
      "metadata": {
        "id": "QTOxoem49ML6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1What is Information Gain, and how is it used in Decision Trees?\n"
      ],
      "metadata": {
        "id": "0gjb22OY9T8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Information Gain (IG) is a key concept used in Decision Trees to decide which feature to split on at each step while building the tree.\n",
        "\n",
        "\n",
        "Information Gain measures how much uncertainty (entropy) in the target variable is reduced after splitting the data based on a particular feature.\n",
        "It tells us how informative a feature is in predicting the target.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Information Gain=Entropy (Parent)-∑i Ni/N*Entropy (Child i)\n",
        "\n",
        "where:\n",
        "\n",
        "Entropy (Parent): Entropy before the split\n",
        "\n",
        "Entropy (Child i):Entropy of each subset after the split\n",
        "\n",
        "Ni: Number of samples in child node i\n",
        "\n",
        "N : Total number of samples before the split\n",
        "\n",
        "Entropy Formula:\n",
        "\n",
        "Entropy measures impurity (randomness) in the dataset.\n",
        "\n",
        "Entropy = -k∑K=1 pk log2(pk)\n",
        "\n",
        "where\n",
        "\n",
        "pk = proportion of samples belonging to class k.\n",
        "\n",
        "If entropy = 0 - all samples are pure (same class).\n",
        "\n",
        "If entropy = 1 - samples are evenly split (high impurity).\n",
        "\n",
        "How It's Used in Decision Trees:\n",
        "\n",
        " - Calculate Entropy of the target before any split.\n",
        "\n",
        " - For each feature, calculate the entropy of the subsets created by splitting on that feature.\n",
        "\n",
        " - Compute Information Gain for each feature.\n",
        "\n",
        " - Select the feature with the highest Information Gain — it gives the most “information” about the target.\n",
        "\n",
        " - Repeat this process recursively for each node."
      ],
      "metadata": {
        "id": "tBpHUrft9YCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2 What is the difference between Gini Impurity and Entropy?\n"
      ],
      "metadata": {
        "id": "HHmVKE3KQBd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Difference between Gini Impurity and Entropy\n",
        "\n",
        "Gini Impurity\n",
        "-------------\n",
        "\n",
        "- Measures the probability of incorrectly classifying a randomly chosen element if it was labeled according to the distribution of labels in the node.\n",
        "\n",
        " - Gini=1-∑pi2\n",
        "\n",
        " - Range - 0 (pure) to 0.5 (for binary split with equal classes)\n",
        " - Lower Gini means higher purity.\n",
        " - Simpler and faster to compute.\n",
        " - Tends to favor larger partitions\n",
        " - Preferred in practice for speed (e.g., used in CART).\n",
        "\n",
        "Entropy\n",
        "-------\n",
        "\n",
        " - Measures the amount of uncertainty (information disorder) in the data; based on information theory.\n",
        "\n",
        " - Entropy=−∑pi​log2​(pi​)\n",
        "\n",
        " - Range - 0 (pure) to 1 (for binary split with equal classes)\n",
        " - Lower Entropy means higher purity.\n",
        " - Slightly slower due to logarithmic computation.\n",
        " - More sensitive to changes in class probabilities.\n",
        " - Preferred when you want a more information-theoretic interpretation\n"
      ],
      "metadata": {
        "id": "xOi0bUg1QV44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3 What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "ZxvTt5KMSAcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Pre-Pruning in Decision Trees (also called Early Stopping) is a technique used to stop the tree from growing too large during its construction — to avoid overfitting and improve generalization.\n",
        "\n",
        "Definition:\n",
        "\n",
        "Pre-pruning stops the splitting process early, before the tree becomes fully grown, based on certain stopping conditions.\n",
        "\n",
        "Common Stopping Criteria:\n",
        "\n",
        "A node will not be split further if any of the following conditions are met:\n",
        "\n",
        "The maximum tree depth is reached.\n",
        "\n",
        "The number of samples in a node is below a minimum threshold (min_samples_split or min_samples_leaf).\n",
        "\n",
        "The impurity decrease (Information Gain or Gini reduction) from a split is too small.\n",
        "\n",
        "The node is already pure (all samples belong to one class).\n",
        "\n",
        "Advantages:\n",
        "\n",
        " - Prevents overfitting (simpler tree, better generalization).\n",
        "\n",
        " - Reduces training time.\n",
        "\n",
        "- Easier to interpret due to smaller tree size.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        " - Risk of underfitting if the pruning is too aggressive.\n",
        "\n",
        " - May miss useful splits that could improve performance later.\n",
        "\n"
      ],
      "metadata": {
        "id": "o_nkExq-Segt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4 :Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical)."
      ],
      "metadata": {
        "id": "1nrJrV8kTnKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load sample dataset (Iris)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print Feature Importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Evaluate model performance (optional)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfh7ie2xTxww",
        "outputId": "aefd9321-9b53-4942-cc2a-496d52984c87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n",
            "\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5 What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "0ItP2kxTUGhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks, but it is most commonly used for classification.\n",
        "\n",
        "Definition:\n",
        "\n",
        "SVM aims to find the best decision boundary (hyperplane) that separates different classes in the feature space with the maximum margin — i.e., the largest possible distance between data points of different classes.\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        " - Hyperplane -\tThe decision boundary that separates different classes. In 2D, it’s a line; in 3D, a plane.\n",
        " - Margin\n",
        "The distance between the hyperplane and the nearest data points from each class. SVM tries to maximize this margin.\n",
        " - Support Vectors\n",
        "The data points closest to the hyperplane that directly influence its position and orientation.\n",
        " - Kernel Trick\n",
        "A mathematical technique that allows SVM to perform classification in non-linear spaces by transforming data into a higher-dimensional space (e.g., using polynomial or RBF kernels).\n",
        "\n",
        "Types of SVM:\n",
        "\n",
        "Linear SVM: Used when data is linearly separable.\n",
        "\n",
        "Non-linear SVM: Uses kernel functions (e.g., RBF, polynomial) when data is not linearly separable."
      ],
      "metadata": {
        "id": "ABQutvUYTrwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6 What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "RLgUqOnAVDrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Kernel Trick in SVM is a mathematical technique that allows the Support Vector Machine to perform non-linear classification by implicitly mapping the input data into a higher-dimensional feature space — without actually computing the transformation.\n",
        "\n",
        "The Idea:\n",
        "--------\n",
        "\n",
        "Many datasets are not linearly separable in their original space.\n",
        "\n",
        "The kernel trick projects data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "Instead of explicitly computing the new coordinates (which is computationally expensive), the kernel trick computes the dot product between pairs of data points in that higher-dimensional space using a kernel function.\n",
        "\n",
        "Mathematical Explanation:\n",
        "\n",
        "If 𝜙(x) is the transformation function to a higher dimension,\n",
        "then instead of computing 𝜙(xi) and 𝜙(xj)\n",
        "\n",
        "SVM uses a kernel function\n",
        "𝐾(𝑥𝑖,𝑥𝑗)=𝜙(𝑥𝑖)⋅𝜙(𝑥𝑗)\n",
        "\n",
        "This saves computation and enables handling complex non-linear boundaries efficiently."
      ],
      "metadata": {
        "id": "krKJWb-5VQtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7 Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "jSt4WhktWumL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracy scores\n",
        "print(\"Accuracy Comparison:\")\n",
        "print(f\"Linear Kernel Accuracy: {acc_linear:.2f}\")\n",
        "print(f\"RBF Kernel Accuracy:    {acc_rbf:.2f}\")\n",
        "\n",
        "# Optional: Compare which kernel performed better\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"\\n Linear kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"\\n RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\n⚖️ Both kernels performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUiISrfpXDbR",
        "outputId": "eec0254c-74c9-439f-e22e-6be279265575"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison:\n",
            "Linear Kernel Accuracy: 0.98\n",
            "RBF Kernel Accuracy:    0.76\n",
            "\n",
            " Linear kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8 What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
      ],
      "metadata": {
        "id": "H9dTPIDwXO_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Naïve Bayes Classifier is a probabilistic supervised learning algorithm based on Bayes' Theorem, primarily used for classification tasks.\n",
        "\n",
        "It predicts the probability that a given data point belongs to a particular class based on the features likelihoods.\n",
        "\n",
        "Bayes Theorem:\n",
        "\n",
        "P(C∣X)=P(X∣C)P(C)/P(X)\n",
        "\n",
        "\n",
        "It's called \"Naïve\" because it assumes that all features are independent of each other given the class label — which is rarely true in real-world data.\n",
        "\n",
        "For example, in text classification, words often depend on each other (e.g., \"not good\"), but Naïve Bayes treats them as independent.\n",
        "\n",
        "Despite this naïve assumption, the algorithm works surprisingly well in practice, especially for high-dimensional data like text.\n",
        "\n",
        "Types of Naïve Bayes Classifiers:\n",
        " - GaussianNB\n",
        " - MultinomialNB\n",
        " - BernoulliNB\n",
        "\n",
        "\t​\n"
      ],
      "metadata": {
        "id": "Z07CNjG6kfW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9 Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes"
      ],
      "metadata": {
        "id": "rXUkpr9XlpNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Comparison of the three main types of Naïve Bayes classifiers\n",
        "\n",
        "1.Gaussian Naïve Bayes (GNB)\n",
        " - Continuous (numeric) features\n",
        " - The features follow a normal (Gaussian) distribution.\n",
        " - Iris dataset (flower measurements), medical data, sensor readings.\n",
        "\n",
        "2.Multinomial Naïve Bayes (MNB)\n",
        " - Discrete or count-based data (non-negative integers)\n",
        " - Features represent counts or frequencies (e.g., word counts)\n",
        " - Text classification (spam detection, topic categorization)\n",
        "\n",
        "3.Bernoulli Naïve Bayes (BNB)\n",
        " - Binary features (0 or 1)\n",
        " - Each feature is a binary variable indicating presence/absence\n",
        " - Document classification with binary features, sentiment analysis.\n"
      ],
      "metadata": {
        "id": "uKddpfx9mWQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10 Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "ZA2p1PUm3Dd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Gaussian Naïve Bayes Classifier Accuracy: {:.2f}\".format(accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlMz187EkeEX",
        "outputId": "2a4ac50b-5e91-44ed-d921-ed19dfe8aef5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Classifier Accuracy: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q2rBU8pPR_uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JCuT_WaPQS8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O0PFDR9HME"
      },
      "outputs": [],
      "source": []
    }
  ]
}